# Week 22-23: Sort & Aggregation

> ORDER BY with in-memory sort, GROUP BY, aggregate functions (COUNT, SUM, AVG, MIN, MAX).

# This Step I Learned

## Blocking vs. Streaming Executors

Previous executors (SeqScan, Filter, Projection) are **streaming**—they process one tuple at a time and pass it immediately to their parent. Sort and Aggregate are **blocking**—they must consume all input before producing any output.

```
Streaming (Filter):          Blocking (Sort):
┌─────────┐                  ┌─────────┐
│ parent  │ ←── one tuple    │ parent  │ ←── tuples start
└────┬────┘                  └────┬────┘     flowing only after
     │                            │          all input consumed
┌────┴────┐                  ┌────┴────┐
│ Filter  │ ←── one tuple    │  Sort   │ ←── materializes ALL
└────┬────┘                  └────┬────┘     tuples in memory
     │                            │
┌────┴────┐                  ┌────┴────┐
│SeqScan  │                  │SeqScan  │
└─────────┘                  └─────────┘
```

Both Sort and Aggregate implement a two-phase pattern:

```rust
async fn next(&mut self) -> Result<Option<Tuple>, ExecutorError> {
    // Phase 1: One-time initialization
    if !self.initialized {
        // Consume all input, materialize, process
        while let Some(tuple) = self.child.next().await? {
            self.buffer.push(tuple);
        }
        self.process();  // sort or compute aggregates
        self.initialized = true;
    }

    // Phase 2: Return materialized results one at a time
    if self.position < self.buffer.len() {
        let tuple = self.buffer[self.position].clone();
        self.position += 1;
        Ok(Some(tuple))
    } else {
        Ok(None)
    }
}
```

## PostgreSQL NULL Ordering

NULL handling in ORDER BY is subtle. PostgreSQL's defaults:
- ASC → NULLS LAST (NULLs sort as "highest")
- DESC → NULLS FIRST (NULLs sort as "highest")

The key insight: NULLs are always treated as the **largest** value, so they appear last in ascending order and first in descending order. This can be overridden with explicit `NULLS FIRST` or `NULLS LAST`.

```rust
pub fn effective_null_ordering(&self) -> NullOrdering {
    match self.nulls {
        NullOrdering::Default => match self.direction {
            SortDirection::Asc => NullOrdering::Last,
            SortDirection::Desc => NullOrdering::First,
        },
        other => other,
    }
}
```

## Hash Aggregation and the GroupKey Challenge

The standard approach for GROUP BY is hash-based aggregation: hash group key values to find/create accumulator buckets. This requires `Hash` and `Eq` implementations for `Value`—but there are two problems:

1. **Floats aren't Hashable**: `f64` doesn't implement `Hash` because `NaN != NaN`
2. **NULL = NULL for grouping**: SQL comparison has `NULL != NULL`, but grouping treats NULLs as equal

The solution is a `GroupKey` wrapper with custom implementations:

```rust
/// Checks if two values are equal for grouping purposes.
/// Unlike SQL comparison where NULL != NULL, for grouping NULL = NULL.
fn values_equal_for_grouping(a: &Value, b: &Value) -> bool {
    match (a, b) {
        (Value::Null, Value::Null) => true,  // ← Different from SQL!
        (Value::Float64(x), Value::Float64(y)) => x.to_bits() == y.to_bits(),
        // ...
    }
}

fn hash_value_for_grouping<H: Hasher>(value: &Value, state: &mut H) {
    match value {
        Value::Float64(f) => f.to_bits().hash(state),  // ← Hash the bits
        // ...
    }
}
```

Using `f64::to_bits()` gives consistent hashing: same bit patterns hash the same, and NaN != NaN at the bit level is handled by the equality check.

## Accumulator State Machine

Each aggregate function maintains its own accumulator state. The tricky cases:

**COUNT(\*) vs COUNT(expr)**:
```rust
Accumulator::Count { count } => {
    // COUNT(*) counts all rows including NULLs
    // COUNT(expr) skips NULLs
    if is_count_star || !value.is_null() {
        *count += 1;
    }
}
```

**SUM type promotion**: SUM of integers returns INT8, but if any float appears, switch to FLOAT8:
```rust
Accumulator::SumInt { sum } => {
    if is_float(value) {
        // Convert to float accumulator mid-stream
        let current = sum.map(|s| s as f64);
        *self = Accumulator::SumFloat {
            sum: Some(current.unwrap_or(0.0) + value_to_f64(value)),
        };
    } else {
        *sum = Some(sum.unwrap_or(0) + n);
    }
}
```

**AVG returns FLOAT8**: Always uses floating-point division, even for integer inputs.

**SUM/AVG of empty set**: Returns NULL, not zero. COUNT returns 0.

## Executor Pipeline Order

The planner builds the executor tree in this order:

```
SeqScan → Filter (WHERE) → Aggregate (GROUP BY + HAVING) → Sort (ORDER BY) → Projection
```

This matches SQL's logical evaluation order:
1. FROM/JOIN (SeqScan)
2. WHERE (Filter) - eliminates rows before grouping
3. GROUP BY/HAVING (Aggregate) - groups and filters groups
4. ORDER BY (Sort) - orders the result
5. SELECT (Projection) - picks columns

Note that HAVING operates on aggregate results, so it's evaluated inside the Aggregate executor after accumulators are finalized—not as a separate Filter.

## Box<dyn Executor> for Heterogeneous Pipelines

Previous executors were purely generic: `Projection<Filter<SeqScan>>`. With optional Sort and Aggregate, the type varies based on the query:
- Simple: `Projection<SeqScan>`
- With WHERE: `Projection<Filter<SeqScan>>`
- With GROUP BY and ORDER BY: `Projection<Sort<Aggregate<Filter<SeqScan>>>>`

Rather than enumerate all combinations, we use trait objects:

```rust
#[async_trait]
impl Executor for Box<dyn Executor> {
    async fn next(&mut self) -> Result<Option<Tuple>, ExecutorError> {
        (**self).next().await
    }
    // ...
}

// In planner:
let filtered: Box<dyn Executor> = if let Some(where_clause) = &stmt.where_clause {
    Box::new(Filter::new(scan, columns.clone(), where_clause.clone()))
} else {
    Box::new(scan)
};
```

The cost is one virtual dispatch per `next()` call—negligible compared to I/O and computation.

# Looking Forward

## External Sort (Not Implemented)

Current implementation materializes all tuples in memory. For large datasets, this hits memory limits. Production systems use **external sort**:

1. Sort chunks that fit in memory
2. Write sorted runs to disk
3. Merge runs using a priority queue

The NOTE comment marks this gap:
```rust
/// NOTE: For production, implement external sort for large datasets.
pub struct Sort<E: Executor> { ... }
```

## COUNT(DISTINCT) (Deferred)

DISTINCT aggregates require tracking unique values per group:

```sql
SELECT dept, COUNT(DISTINCT job) FROM employees GROUP BY dept;
```

This needs a HashSet per group, significantly increasing memory usage. The planner currently rejects this:

```rust
if *distinct && func != AggregateFunction::Count {
    return Err(ExecutorError::Unsupported {
        feature: format!("{} DISTINCT", func.name().to_uppercase()),
    });
}
```

## Non-Aggregate Columns in GROUP BY

Standard SQL allows selecting non-aggregated columns if they're functionally dependent on GROUP BY columns:

```sql
-- Valid if user_id is PRIMARY KEY:
SELECT user_id, username, COUNT(*) FROM users GROUP BY user_id;
```

Current implementation doesn't validate this—it accepts any column reference. Production systems either:
- Reject non-aggregated columns not in GROUP BY (strict mode)
- Infer functional dependencies from PRIMARY KEY/UNIQUE constraints

## Arbitrary GROUP BY Output Order

HashMap iteration order is unspecified:

```rust
// NOTE: The output is in arbitrary order (HashMap iteration order).
// If ORDER BY is needed, it should be applied via a Sort executor.
for (key_values, accumulators) in self.groups.values() { ... }
```

This is correct behavior—GROUP BY without ORDER BY has no guaranteed order. But it means identical queries may return rows in different orders across runs.

## Production Readiness Gaps

### Memory Limits
No backpressure or spill-to-disk. Large aggregations can OOM the process. Solutions:
- External aggregation (hash partitioning to disk)
- Memory budget enforcement with spilling

### Numeric Precision
SUM/AVG use Rust's native `i64`/`f64`. For financial applications:
- `NUMERIC`/`DECIMAL` types with arbitrary precision
- Overflow checking for integer sums

### Partial Aggregation
Current implementation reads all input before outputting. Streaming aggregates could output partial results for:
- Progressive UI updates
- Cancellation without losing work

### LIMIT Optimization
`ORDER BY x LIMIT 10` shouldn't sort the entire dataset. A **top-N heap** only keeps the 10 largest/smallest values, O(n log k) instead of O(n log n).
