# Week 16-17: System Catalog

> Store table/column definitions as heap tuples with MVCC. Bootstrap reserved catalog tables (sys_tables, sys_columns - similar concepts to PostgreSQL's pg_class/pg_attribute). Implement auto-increment sequences for SERIAL columns.

# This Step I Learned

## Catalog: Self-Describing Metadata

The system catalog stores metadata about tables as tuples in heap pages.

### The Bootstrap Problem

To read those tuples, we need to know the table schema—which is stored in the catalog itself. This creates a circular dependency that must be carefully broken.

```text
To read sys_tables:
  -> Need sys_tables schema
  -> Schema is in sys_columns
  -> To read sys_columns...
  -> Need sys_columns schema
  -> ...
```

Our solution: **Hardcode catalog schemas in Rust code**, then use them to interpret catalog pages.

```rust
impl SystemCatalogTable for TableInfo {
    const TABLE_ID: u32 = 1;
    const TABLE_NAME: &str = "sys_tables";
    const COLUMN_NAMES: &[&str] = &["table_id", "table_name", "first_page"];
    const SCHEMA: &[i32] = &[type_oid::INT4, type_oid::TEXT, type_oid::INT8];
}
```

The `SystemCatalogTable` trait provides a uniform interface for all catalog tables, ensuring schema definitions stay synchronized with serialization/deserialization logic.

### `sys_` Tables

Despite hardcoding schemas for bootstrap, we still insert catalog metadata into the catalog tables themselves. This enables:

1. **Introspection**: `SELECT * FROM sys_tables` works like any user table
2. **MVCC visibility**: Catalog changes follow normal transaction rules
3. **Consistency**: One source of truth once the system is running

Bootstrap creates three tables:

| Table Name      | Rust Type        | Description                     |
| --------------- | ---------------- | ------------------------------- |
| `sys_tables`    | [`TableInfo`]    | Table metadata (id, name, page) |
| `sys_columns`   | [`ColumnInfo`]   | Column metadata per table       |
| `sys_sequences` | [`SequenceInfo`] | SERIAL column sequences         |

### Superblock: Database Metadata

Page 0 is reserved as the [**superblock**](../src/catalog/superblock.rs), storing essential database metadata.

The superblock is outside the WAL system (it may contain WAL metadata itself in future steps), so changes are flushed with immediate fsync.

```text
Page 0          Heap Pages
+------------+  +-------------+  +---------------+  +----------------+
| Superblock |  | sys_tables  |  | sys_columns   |  | sys_sequences  |
+------------+  +-------------+  +---------------+  +----------------+
      |
      +-> Catalog Page IDs + ID Generators
```

## Sequences: MVCC Bypass

Sequences (SERIAL columns) require special handling. Following PostgreSQL's design choice, sequences are **never rolled back**—even if a transaction aborts, consumed sequence values are gone. This prevents:

1. Primary key conflicts when retrying failed transactions
2. Unpredictable gaps that would confuse users expecting rollback

We implement this via **in-place updates**:

```rust
pub async fn nextval(&self, seq_id: u32) -> Result<i64, CatalogError> {
    // Page latch held during entire operation for atomicity
    let mut page = HeapPage::new(self.pool.fetch_page_mut(...).await?);

    // Find sequence, increment, update in-place (bypassing MVCC)
    seq.next_val += 1;
    page.update_record_in_place(slot_id, &seq.to_record())?;

    Ok(current_val)
}
```

The `update_record_in_place` method modifies only the record data, leaving the TupleHeader unchanged. This works because:

- Sequence name never changes (fixed-size TEXT in our simple implementation)
- next_val is always i64 (fixed size)
- No MVCC versioning needed—changes are immediately visible to all

# Looking Forward

## Future Steps

- **Step 10 (Executor)**: Will use catalog lookups to resolve table names and build scan plans
- **Step 11 (DML)**: INSERT will call `nextval` for SERIAL columns automatically
- **Step 15 (VACUUM)**: Catalog pages need vacuuming like any heap page

## Deferred: Multi-Page Catalog Tables

Each catalog table currently uses a single page:

```rust
// create_table allocates one page per table
let first_page = {
    let page_guard = self.pool.new_page().await?;
    HeapPage::new(page_guard).init();
    page_guard.page_id()
};
```

With ~8KB pages and typical metadata size, this limits tables to roughly:

- sys_tables: ~100 tables
- sys_columns: ~200 columns total

This will be addressed in Step 15 when we implement FSM (Free Space Map) for efficient page selection.

## Deferred: Catalog Caching

Every table lookup currently scans the sys_tables page:

```rust
pub async fn get_table(&self, snapshot: &Snapshot, name: &str) -> ... {
    let page = HeapPage::new(self.pool.fetch_page(...).await?);
    for (_slot_id, header, record) in page.scan(TableInfo::SCHEMA) {
        if info.table_name == name { return Ok(Some(info)); }
    }
}
```

This is `O(n)` per lookup. Production systems maintain an in-memory cache invalidated on DDL operations. We intentionally deferred this to keep the implementation simple and to see actual access patterns before designing a caching strategy.

## Production Readiness Gaps

### Atomic DDL

`create_table` is not atomic—a crash mid-operation leaves orphaned pages:

1. Allocate table ID and persist superblock
2. Create heap page
3. Insert into sys_tables
4. Insert into sys_columns
5. **Crash here** → sys_tables entry exists but columns incomplete

WAL (Step 13) will provide transactional DDL by logging all operations.

### Concurrent DDL Protection

The check-then-insert pattern for table creation has a race condition:

```rust
// Thread 1 and Thread 2 both reach here simultaneously
if let Some(table) = self.get_table(&snapshot, &stmt.name).await? {
    return Err(TableAlreadyExists);
}
// Both threads pass the check, both create tables with same name
let table_id = self.superblock.write().allocate_table_id();
```

Production systems use:

- Catalog-level locks (AccessExclusiveLock on sys_tables)
- Or unique index constraints that enforce atomicity at the storage level

### DROP TABLE

Not implemented—we can create tables but not remove them. DROP requires:

- Marking tuples in sys_tables/sys_columns as deleted (set xmax)
- Deallocating table's heap pages
- Handling FK constraints and dependencies
