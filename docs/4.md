# Week 5-6: Storage Layer

> Define the Storage trait. Implement MemoryStorage and FileStorage for 8KB page I/O.

# This Week I Learned

## Storage Trait: Page I/O Interface

Implemented the `Storage` trait as the foundation for page-based I/O operations:

```rust
pub trait Storage: Send + Sync {
    async fn read_page(&self, page_id: PageId, buf: &mut [u8]) -> Result<(), StorageError>;
    async fn write_page(&self, page_id: PageId, buf: &[u8]) -> Result<(), StorageError>;
    async fn allocate_page(&self) -> Result<PageId, StorageError>;
    async fn page_count(&self) -> usize;
    async fn sync_all(&self) -> Result<(), StorageError>;
}
```

**Key Characteristics:**

- **Send + Sync**: Thread-safe for concurrent access from multiple async tasks. The Buffer Pool Manager (Week 7-8) will use this trait from multiple connections.
- **Caller-owned buffers**: Storage doesn't allocate memory - the caller provides `&mut [u8]` for reads and `&[u8]` for writes. All buffers must be exactly `PAGE_SIZE` (8KB).
- **No caching**: This layer performs raw I/O only. Caching is the Buffer Pool Manager's responsibility.

## PageId and PAGE_SIZE

- **PAGE_SIZE**: 8192 bytes (8KB), aligned with OS page sizes. Intentionally using the same page size as PostgreSQL for learning.
- **PageId**: Simple `u64` wrapper that uniquely identifies a page. Provides `byte_offset()` method to calculate file position (`page_num * 8192`).

Notice that we use 8KB (8192 bytes) fixed-size pages. This is because...

- Uses the same page size as PostgreSQL for learning purposes
- Aligns with common OS page sizes (4KB multiple)

## PageData: Page-Aligned Memory Allocation

Implemented `PageData` for low-level page-aligned memory management:

- **4KB alignment**: Uses `std::alloc` to allocate 8KB blocks aligned to 4KB boundaries (typical OS page size).
- **Zero-initialized**: New pages are zeroed on allocation for safety.
- **Send + Sync**: Safe to share across threads (used inside `Mutex<Vec<PageData>>` in MemoryStorage).

**Why page alignment?** Aligning to OS page boundaries provides better cache efficiency and compatibility with future Direct I/O features.

## Why Async Rust for Storage?

RDBMS implementations face a fundamental concurrency challenge: handling multiple client connections while managing shared resources (buffer pool, locks, disk I/O). There are several established approaches:

| Concurrency Model          | Examples                   | Strengths                                                       | Weaknesses                                                  |
| -------------------------- | -------------------------- | --------------------------------------------------------------- | ----------------------------------------------------------- |
| **Process-per-connection** | PostgreSQL (traditional)   | Full isolation, crash safety                                    | High memory overhead, slow context switching                |
| **Thread-per-connection**  | MySQL (traditional)        | Lighter than processes, shared memory                           | Doesn't scale with many connections, context switching cost |
| **Thread pool**            | Modern PostgreSQL, MariaDB | Bounded resource usage, better scalability                      | Complex work stealing, still limited by thread count        |
| **Async/Event-driven**     | ScyllaDB, Redis            | Handle many connections with few threads, efficient I/O waiting | Complex async programming, harder debugging                 |

Our approach corresponds to the **Async/Event-driven** model, but leverages Rust's safety guarantees and tokio's ecosystem:

1. **Compile-time `Send + Sync` checking**: The compiler prevents holding non-Send values (e.g., `Rc<T>`) across `.await` points. This is enforced at compile time, unlike runtime checks in Go or dynamic languages.
2. **Strongly-typed futures**: `Future<Output = Result<T, E>>` encodes return types in the type system. C-based async systems (libuv callbacks) often use void pointers; Rust futures are fully type-checked.
3. **Integrated ecosystem**: `tokio::fs::File`, `tokio::net::TcpListener`, and async traits work together without adapters. No bridging layer needed between different async primitives.

# Looking Forward

## Buffer Pool Manager Integration

The `Storage` trait is designed for integration with the Buffer Pool Manager (Week 7-8).

```
┌─────────────────────────────┐
│ Buffer Pool Manager         │  ← Week 7-8
│ - fetch_page(PageId)        │
│ - unpin_page(PageId)        │
│ - LRU eviction              │
│ - Arc<RwLock<Page>>         │
└───────────┬─────────────────┘
            │ read_page / write_page
            │ (on cache miss / eviction)
            v
┌─────────────────────────────┐
│ Storage Trait               │  ← Week 5-6 (this)
│ - No caching                │
│ - Raw 8KB I/O               │
└───────────┬─────────────────┘
            v
      MemoryStorage / FileStorage
```

- **Week 7-8: Buffer Pool Manager** will:
  - Call `storage.read_page()` on cache miss
  - Call `storage.write_page()` when evicting dirty pages
  - Maintain page-level locks (`Arc<RwLock<Page>>`) for concurrency control
  - Implement LRU eviction policy
- **Week 9: Slotted Page Structure** will use `Storage` (via BPM) to persist variable-length records within 8KB pages.
- **Week 21: Write-Ahead Log (WAL)** relies on `sync_all()` to ensure durability: logs must be flushed to disk before data pages.

## Production Readiness Gaps

The current implementation prioritizes learning and clarity. For production use, consider:

- **Graceful shutdown**: Flush all dirty pages and sync before exit
- **Corruption recovery**: Checksum validation (e.g., CRC32) for page integrity
- **I/O error handling**: Retry logic for transient failures
- **Resource limits**: Maximum file size, memory limits
- **Multiple files**: Support for tablespaces, indexes in separate files
- **Direct I/O**: Bypass OS page cache for more control (e.g., `O_DIRECT` on Linux)
- **Concurrent file handles**: One handle per thread for parallel I/O (pread/pwrite)
- **Monitoring**: I/O metrics (read/write latency, throughput, cache hit rate)
